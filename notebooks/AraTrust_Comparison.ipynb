{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AraTrust Evaluation Comparison\n",
    "\n",
    "This notebook fetches evaluation data from Langfuse, saves datasets per run, and generates comparison visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from langfuse import Langfuse\n",
    "from src.config import Config\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "langfuse = Langfuse(\n",
    "    host=config.LANGFUSE_HOST,\n",
    "    public_key=config.LANGFUSE_PUBLIC_KEY,\n",
    "    secret_key=config.LANGFUSE_SECRET_KEY,\n",
    ")\n",
    "\n",
    "OUTPUT_DIR = project_root / 'results' / 'runs'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "langfuse.auth_check()\n",
    "print(f'Connected to Langfuse at {config.LANGFUSE_HOST}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_traces(limit_per_page=100):\n",
    "    all_traces = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        traces = langfuse.fetch_traces(limit=limit_per_page, page=page)\n",
    "        if not traces.data:\n",
    "            break\n",
    "        all_traces.extend(traces.data)\n",
    "        page += 1\n",
    "    return all_traces\n",
    "\n",
    "def get_all_runs(min_traces=500):\n",
    "    print('Fetching all traces from Langfuse...')\n",
    "    all_traces = fetch_all_traces()\n",
    "    print(f'Total traces fetched: {len(all_traces)}')\n",
    "    \n",
    "    sessions = defaultdict(list)\n",
    "    for trace in all_traces:\n",
    "        if trace.session_id:\n",
    "            sessions[trace.session_id].append(trace)\n",
    "    \n",
    "    valid_runs = {k: v for k, v in sessions.items() if len(v) >= min_traces}\n",
    "    \n",
    "    print(f'Found {len(valid_runs)} valid runs:')\n",
    "    for run_id, traces in sorted(valid_runs.items()):\n",
    "        model_name = traces[0].tags[1] if traces and traces[0].tags and len(traces[0].tags) > 1 else 'unknown'\n",
    "        print(f'  - {run_id}: {len(traces)} traces (model: {model_name})')\n",
    "    \n",
    "    return valid_runs\n",
    "\n",
    "def extract_run_data(traces, run_id):\n",
    "    model_name = traces[0].tags[1] if traces and traces[0].tags and len(traces[0].tags) > 1 else run_id\n",
    "    \n",
    "    records = []\n",
    "    for trace in traces:\n",
    "        if not trace.metadata:\n",
    "            continue\n",
    "        records.append({\n",
    "            'run_id': run_id,\n",
    "            'model': model_name,\n",
    "            'trace_id': trace.id,\n",
    "            'sample_idx': trace.metadata.get('sample_idx'),\n",
    "            'category': trace.metadata.get('category'),\n",
    "            'subcategory': trace.metadata.get('subcategory'),\n",
    "            'correct_answer': trace.metadata.get('correct_answer'),\n",
    "            'predicted': trace.output.get('predicted') if trace.output else None,\n",
    "            'is_correct': trace.metadata.get('is_correct'),\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "print('Functions defined!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = get_all_runs(min_traces=500)\n",
    "\n",
    "run_dataframes = {}\n",
    "for run_id, traces in runs.items():\n",
    "    print(f'Processing: {run_id}')\n",
    "    df = extract_run_data(traces, run_id)\n",
    "    run_dataframes[run_id] = df\n",
    "    \n",
    "    safe_run_id = run_id.replace('/', '_').replace(' ', '_')\n",
    "    df.to_csv(OUTPUT_DIR / f'run_{safe_run_id}.csv', index=False)\n",
    "    print(f'  Saved {len(df)} records, Accuracy: {df[\"is_correct\"].mean()*100:.2f}%')\n",
    "\n",
    "print(f'Saved {len(run_dataframes)} runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat(run_dataframes.values(), ignore_index=True)\n",
    "\n",
    "# IMPORTANT: Drop rows with None/NaN category to avoid sorting errors\n",
    "all_data = all_data.dropna(subset=['category'])\n",
    "\n",
    "print(f'Total records: {len(all_data)}')\n",
    "print('\\nModels:')\n",
    "for model in all_data['model'].unique():\n",
    "    acc = all_data[all_data['model'] == model]['is_correct'].mean() * 100\n",
    "    print(f'  - {model}: {acc:.2f}%')\n",
    "\n",
    "category_accuracy = all_data.groupby(['model', 'category'])['is_correct'].mean().reset_index()\n",
    "category_accuracy['accuracy'] = category_accuracy['is_correct'] * 100\n",
    "\n",
    "subcategory_accuracy = all_data.groupby(['model', 'category', 'subcategory'])['is_correct'].mean().reset_index()\n",
    "subcategory_accuracy['accuracy'] = subcategory_accuracy['is_correct'] * 100\n",
    "\n",
    "overall_accuracy = all_data.groupby('model')['is_correct'].mean().reset_index()\n",
    "overall_accuracy['accuracy'] = overall_accuracy['is_correct'] * 100\n",
    "overall_accuracy = overall_accuracy.sort_values('accuracy', ascending=False)\n",
    "\n",
    "print('\\nData prepared!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category Comparison Bar Chart\n",
    "category_pivot = category_accuracy.pivot(index='category', columns='model', values='accuracy')\n",
    "n_models = len(category_pivot.columns)\n",
    "n_categories = len(category_pivot.index)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "x = np.arange(n_categories)\n",
    "width = 0.8 / n_models\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, n_models))\n",
    "\n",
    "for i, (model, color) in enumerate(zip(category_pivot.columns, colors)):\n",
    "    offset = (i - n_models/2 + 0.5) * width\n",
    "    bars = ax.bar(x + offset, category_pivot[model], width, label=model, color=color, edgecolor='black')\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        if not np.isnan(height):\n",
    "            ax.annotate(f'{height:.1f}%', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                       xytext=(0, 3), textcoords='offset points', ha='center', fontsize=7, rotation=90)\n",
    "\n",
    "ax.set_xlabel('Category', fontweight='bold')\n",
    "ax.set_ylabel('Accuracy (%)', fontweight='bold')\n",
    "ax.set_title('AraTrust: Category Accuracy by Model', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(category_pivot.index, rotation=45, ha='right')\n",
    "ax.set_ylim(0, 110)\n",
    "ax.legend(title='Model', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "ax.axhline(y=90, color='green', linestyle='--', alpha=0.5)\n",
    "ax.axhline(y=70, color='orange', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'category_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subcategory Heatmap\n",
    "subcategory_pivot = subcategory_accuracy.pivot_table(\n",
    "    index=['category', 'subcategory'], columns='model', values='accuracy'\n",
    ")\n",
    "\n",
    "fig_height = max(10, len(subcategory_pivot) * 0.4)\n",
    "fig, ax = plt.subplots(figsize=(12, fig_height))\n",
    "\n",
    "sns.heatmap(subcategory_pivot, annot=True, fmt='.1f', cmap='RdYlGn',\n",
    "            center=75, vmin=0, vmax=100, linewidths=0.5, ax=ax,\n",
    "            cbar_kws={'label': 'Accuracy (%)'})\n",
    "\n",
    "ax.set_title('AraTrust: Subcategory Accuracy Heatmap', fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'subcategory_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Accuracy Bar Chart\n",
    "fig, ax = plt.subplots(figsize=(10, max(4, len(overall_accuracy) * 0.8)))\n",
    "overall_sorted = overall_accuracy.sort_values('accuracy', ascending=True)\n",
    "colors = plt.cm.RdYlGn(overall_sorted['accuracy'] / 100)\n",
    "\n",
    "bars = ax.barh(overall_sorted['model'], overall_sorted['accuracy'], color=colors, edgecolor='black')\n",
    "for bar, acc in zip(bars, overall_sorted['accuracy']):\n",
    "    ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,\n",
    "            f'{acc:.2f}%', va='center', fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Accuracy (%)', fontweight='bold')\n",
    "ax.set_ylabel('Model', fontweight='bold')\n",
    "ax.set_title('AraTrust: Overall Accuracy by Model', fontweight='bold')\n",
    "ax.set_xlim(0, 105)\n",
    "ax.axvline(x=90, color='green', linestyle='--', alpha=0.7)\n",
    "ax.axvline(x=80, color='orange', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'overall_accuracy.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Summary Table\n",
    "summary_data = []\n",
    "\n",
    "for model in all_data['model'].unique():\n",
    "    model_data = all_data[all_data['model'] == model]\n",
    "    \n",
    "    row = {\n",
    "        'Model': model,\n",
    "        'Total Samples': len(model_data),\n",
    "        'Correct': int(model_data['is_correct'].sum()),\n",
    "        'Incorrect': int((~model_data['is_correct']).sum()),\n",
    "        'Overall Accuracy (%)': model_data['is_correct'].mean() * 100,\n",
    "    }\n",
    "    \n",
    "    # FIXED: Filter out None/NaN categories before sorting\n",
    "    valid_categories = model_data['category'].dropna().unique()\n",
    "    for category in sorted(valid_categories):\n",
    "        cat_data = model_data[model_data['category'] == category]\n",
    "        row[f'{category} (%)'] = cat_data['is_correct'].mean() * 100\n",
    "    \n",
    "    summary_data.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Overall Accuracy (%)', ascending=False)\n",
    "\n",
    "# Display\n",
    "format_dict = {'Overall Accuracy (%)': '{:.2f}'}\n",
    "format_dict.update({col: '{:.2f}' for col in summary_df.columns if col.endswith('(%)')})\n",
    "\n",
    "styled_df = summary_df.style.format(format_dict).background_gradient(\n",
    "    subset=['Overall Accuracy (%)'], cmap='RdYlGn', vmin=60, vmax=100\n",
    ")\n",
    "display(styled_df)\n",
    "\n",
    "summary_df.to_csv(OUTPUT_DIR / 'comparison_summary.csv', index=False)\n",
    "print(f'Saved to {OUTPUT_DIR / \"comparison_summary.csv\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Two Models\n",
    "def compare_models(model1, model2):\n",
    "    df1 = all_data[all_data['model'] == model1]\n",
    "    df2 = all_data[all_data['model'] == model2]\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "    print(f'Comparison: {model1} vs {model2}')\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    acc1 = df1['is_correct'].mean() * 100\n",
    "    acc2 = df2['is_correct'].mean() * 100\n",
    "    print(f'\\nOverall: {model1}={acc1:.2f}% | {model2}={acc2:.2f}% | Diff={acc2-acc1:+.2f}%')\n",
    "    \n",
    "    print(f\"\\n{'Category':<25} {model1[:12]:>12} {model2[:12]:>12} {'Change':>10}\")\n",
    "    print('-' * 65)\n",
    "    \n",
    "    # FIXED: Filter None categories\n",
    "    all_cats = set(df1['category'].dropna().unique()) | set(df2['category'].dropna().unique())\n",
    "    \n",
    "    for cat in sorted(all_cats):\n",
    "        a1 = df1[df1['category'] == cat]['is_correct'].mean() * 100 if len(df1[df1['category'] == cat]) > 0 else 0\n",
    "        a2 = df2[df2['category'] == cat]['is_correct'].mean() * 100 if len(df2[df2['category'] == cat]) > 0 else 0\n",
    "        diff = a2 - a1\n",
    "        status = 'UP' if diff > 0 else ('DOWN' if diff < 0 else '')\n",
    "        print(f'{cat:<25} {a1:>11.2f}% {a2:>11.2f}% {diff:>+9.2f}% {status}')\n",
    "\n",
    "models = all_data['model'].unique().tolist()\n",
    "print(f'Available models: {models}')\n",
    "if len(models) >= 2:\n",
    "    compare_models(models[0], models[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
